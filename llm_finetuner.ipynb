{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description: Model Fine-tuning (Unsloth) and Quantization (GGUF)\n",
    "## Author: \"Mostafa Shams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0273eb1",
   "metadata": {
    "_cell_guid": "04d432d1-bb98-458a-a842-2fd5ab6ad83d",
    "_uuid": "0d22e1a2-fb29-4291-aa95-20f34717720a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-19T21:04:58.928077Z",
     "iopub.status.busy": "2025-11-19T21:04:58.927568Z",
     "iopub.status.idle": "2025-11-19T21:07:27.796653Z",
     "shell.execute_reply": "2025-11-19T21:07:27.795579Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 148.874128,
     "end_time": "2025-11-19T21:07:27.798128",
     "exception": false,
     "start_time": "2025-11-19T21:04:58.924000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m246.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m200.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m332.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m283.5/283.5 kB\u001b[0m \u001b[31m351.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m272.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m349.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m318.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m209.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m167.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m164.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m136.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m239.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m144.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m150.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m155.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m134.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m139.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m331.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m317.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\r\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\r\n",
      "s3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.9.0 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\r\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m247.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h GPU Active: Tesla T4 | VRAM: 14.74 GB\n",
      "Collecting llama-cpp-python\r\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m298.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\r\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\r\n",
      "  Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m237.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\r\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\r\n",
      "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\r\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m238.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m303.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m208.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m274.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\r\n",
      "Building wheels for collected packages: llama-cpp-python\r\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\r\n",
      "  \r\n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mBuilding wheel for llama-cpp-python \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\r\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\r\n",
      "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\r\n",
      "  \r\n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25herror\r\n",
      "\u001b[31m  ERROR: Failed building wheel for llama-cpp-python\u001b[0m\u001b[31m\r\n",
      "\u001b[0mFailed to build llama-cpp-python\r\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (llama-cpp-python)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "!pip install -q --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q --no-cache-dir --no-deps xformers trl peft accelerate bitsandbytes\n",
    "!pip install -q evaluate sacrebleu rouge_score\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU Active: {gpu_name} | VRAM: {vram:.2f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"No GPU detected. Ensure Accelerator is set to GPU T4 x2.\")\n",
    "\n",
    "!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install -U --force-reinstall --no-cache-dir llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a369974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:07:27.829068Z",
     "iopub.status.busy": "2025-11-19T21:07:27.828044Z",
     "iopub.status.idle": "2025-11-19T21:07:30.883250Z",
     "shell.execute_reply": "2025-11-19T21:07:30.882293Z"
    },
    "papermill": {
     "duration": 3.071693,
     "end_time": "2025-11-19T21:07:30.884426",
     "exception": false,
     "start_time": "2025-11-19T21:07:27.812733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting Data Sanitization...\n",
      "Data Loaded Successfully!\n",
      "   - Total Valid Rows: 2227\n",
      "   - Skipped/Corrupt:  0\n",
      "   - Train Size:       2004\n",
      "   - Test Size:        223\n",
      "\n",
      "ğŸ” DEBUG (First Row Sample):\n",
      "\n",
      "<|im_start|>system\n",
      "You are a helpful medical assistant. You provide information in Arabic<|im_end|>\n",
      "<|im_start|>user\n",
      "Ø¥ÙŠÙ‡ Ù‡Ùˆ ÙØ­Øµ ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ÙƒØ¨Ø¯ (Liver Function Tests)ØŸ<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Ù‡ÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¯Ù… Ø§Ù„ØªÙŠ ØªÙ‚ÙŠØ³ Ù…Ø³ØªÙˆÙŠØ§Øª Ø¥Ù†Ø²ÙŠÙ…Ø§Øª ÙˆØ¨Ø±ÙˆØªÙŠÙ†Ø§Øª Ù…Ø¹ÙŠÙ†Ø© ÙÙŠ Ø§Ù„Ø¯Ù… ÙŠÙ†ØªØ¬Ù‡Ø§ Ø§Ù„ÙƒØ¨Ø¯. ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ³Ø§Ø¹Ø¯...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "INPUT_FILE = \"/kaggle/input/medical/medicalAR_fixed.jsonl\"\n",
    "formatted_rows = []\n",
    "skipped_count = 0\n",
    "\n",
    "print(f\"Starting Data Sanitization...\")\n",
    "\n",
    "def format_chatml(messages):\n",
    "    text = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg.get(\"role\", \"\").strip()\n",
    "        content = msg.get(\"content\", \"\").strip()\n",
    "        if not role or not content or not isinstance(content, str):\n",
    "            return None \n",
    "        text += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "    return text + \"<|im_start|>assistant\\n\"\n",
    "\n",
    "try:\n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                if \"root\" in data and \"messages\" in data[\"root\"]:\n",
    "                    messages = data[\"root\"][\"messages\"]\n",
    "                elif \"messages\" in data:\n",
    "                    messages = data[\"messages\"]\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "                    continue \n",
    "\n",
    "                roles = [m['role'] for m in messages]\n",
    "                if 'user' in roles and 'assistant' in roles:\n",
    "                    chat_text = format_chatml(messages)\n",
    "                    if chat_text:\n",
    "                        formatted_rows.append({\n",
    "                            \"text\": chat_text, \n",
    "                            \"ground_truth\": messages[-1]['content']\n",
    "                        })\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Line {i} Error: {e}\")\n",
    "                skipped_count += 1\n",
    "\n",
    "    full_dataset = Dataset.from_pandas(pd.DataFrame(formatted_rows))\n",
    "    dataset_split = full_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = dataset_split[\"train\"]\n",
    "    test_dataset = dataset_split[\"test\"]\n",
    "\n",
    "    print(f\"Data Loaded Successfully!\")\n",
    "    print(f\"Total Valid Rows: {len(full_dataset)}\")\n",
    "    print(f\"Skipped/Corrupt:  {skipped_count}\")\n",
    "    print(f\"Train Size:       {len(train_dataset)}\")\n",
    "    print(f\"Test Size:        {len(test_dataset)}\")\n",
    "    \n",
    "    print(\"\\nFirst Row Sample:\")\n",
    "    print(train_dataset[0]['text'][:300] + \"...\\n\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: File not found. Check path!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284c70c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:07:30.914624Z",
     "iopub.status.busy": "2025-11-19T21:07:30.914243Z",
     "iopub.status.idle": "2025-11-19T21:08:40.883055Z",
     "shell.execute_reply": "2025-11-19T21:08:40.882334Z"
    },
    "papermill": {
     "duration": 69.984954,
     "end_time": "2025-11-19T21:08:40.884403",
     "exception": false,
     "start_time": "2025-11-19T21:07:30.899449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 21:07:41.211000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763586461.600394      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763586461.707755      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[xformers|WARNING]WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.9.0+cu128 with CUDA 1208 (you have 2.6.0+cu124)\n",
      "    Python  3.10.19 (you have 3.11.13)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "Unsloth: Xformers was not installed correctly.\n",
      "Please install xformers separately first.\n",
      "Then confirm if it's correctly installed by running:\n",
      "python -m xformers.info\n",
      "\n",
      "Longer error message:\n",
      "xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.9.0+cu128 with CUDA 1208 (you have 2.6.0+cu124)\n",
      "    Python  3.10.19 (you have 3.11.13)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen 2.5-3B-Instruct (4-bit)...\n",
      "==((====))==  Unsloth 2025.11.3: Fast Qwen2 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf01987836d43389f61800b3aa811bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68647bd34e648b1bfbbac4bbb35c9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3316cb122e43b59627e122e29f5ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4aa38faed3b49ad999e9d9949ed98c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04964af3ea174d7192bc66331d6eb6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fa1c820eab4041997224cf8eb07fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ac6081a21445dbbd5bd7b58c305084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f0c7e0298446f8962d95b640ac0b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.3 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Loaded & Adapters Applied.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 1024 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "print(\"Loading Qwen 2.5-3B-Instruct (4-bit)...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\", \n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "print(\"Model Loaded & Adapters Applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae12f10d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:08:40.918208Z",
     "iopub.status.busy": "2025-11-19T21:08:40.917517Z",
     "iopub.status.idle": "2025-11-19T21:32:05.468349Z",
     "shell.execute_reply": "2025-11-19T21:32:05.467487Z"
    },
    "papermill": {
     "duration": 1404.585231,
     "end_time": "2025-11-19T21:32:05.486099",
     "exception": false,
     "start_time": "2025-11-19T21:08:40.900868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687cd773fb5f4f72a1c05b61cb5edc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eddcc47633d4c0ebcd1e1711a759872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53af168800314cceb4db6de867077ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d679f88520bd40208676c4454bbbf4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase A: Running Baseline Inference...\n",
      "running inference on 200 samples...\n",
      "Phase A Scores: {'bleu': 0.029, 'rouge1': 0.058}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "def calculate_metrics(preds, refs):\n",
    "    preds = [p if len(p) > 0 else \" \" for p in preds]\n",
    "    r = rouge.compute(predictions=preds, references=refs)\n",
    "    b = bleu.compute(predictions=preds, references=refs)\n",
    "    return {\"bleu\": round(b['bleu'], 4), \"rouge1\": round(r['rouge1'], 4)}\n",
    "\n",
    "def inference_loop(model_engine, tokenizer, dataset, limit=200, is_gguf=False):\n",
    "    preds, refs = [], []\n",
    "    print(f\"Running inference on {limit} samples...\")\n",
    "    \n",
    "    for i in range(limit):\n",
    "        row = dataset[i]\n",
    "        prompt = row['text'].split(\"<|im_start|>assistant\")[0] + \"<|im_start|>assistant\\n\"\n",
    "        ref = row['ground_truth']\n",
    "        \n",
    "        if not is_gguf:\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            outputs = model_engine.generate(**inputs, max_new_tokens=100, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
    "            decoded = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "        else:\n",
    "            out = model_engine(prompt, max_tokens=100, stop=[\"<|im_end|>\"], echo=False)\n",
    "            decoded = out['choices'][0]['text']\n",
    "            \n",
    "        preds.append(decoded.strip())\n",
    "        refs.append(ref.strip())\n",
    "    return preds, refs\n",
    "\n",
    "print(\"Running Baseline Inference...\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "preds_a, refs_a = inference_loop(model, tokenizer, test_dataset, limit=200)\n",
    "scores_a = calculate_metrics(preds_a, refs_a)\n",
    "print(f\"Baseline Scores: {scores_a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5e58d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:32:05.518524Z",
     "iopub.status.busy": "2025-11-19T21:32:05.518257Z",
     "iopub.status.idle": "2025-11-19T21:56:59.432145Z",
     "shell.execute_reply": "2025-11-19T21:56:59.431253Z"
    },
    "papermill": {
     "duration": 1493.93147,
     "end_time": "2025-11-19T21:56:59.433363",
     "exception": false,
     "start_time": "2025-11-19T21:32:05.501893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting Full Production Training...\n",
      "   - Training Set Size: 2004 examples\n",
      "   - Target Epochs: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2538c93995064dd0b0f59628eca634cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/2004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Total Estimated Steps: ~375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,004 | Num Epochs = 3 | Total steps = 189\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 29,933,568 of 3,115,872,256 (0.96% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 24:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.733800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.041900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.866400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.664600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.527600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Full Training Complete.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"Starting Full Production Training...\")\n",
    "print(f\"Training Set Size: {len(train_dataset)} examples\")\n",
    "print(f\"Target Epochs: 3\")\n",
    "\n",
    "model.train() \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, \n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,  \n",
    "        gradient_accumulation_steps = 4,  \n",
    "        learning_rate = 2e-4,             \n",
    "        num_train_epochs = 3,             \n",
    "        max_steps = -1,                   \n",
    "        warmup_ratio = 0.05,              \n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        optim = \"adamw_8bit\",             \n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        logging_steps = 25,               \n",
    "        save_strategy = \"epoch\",          \n",
    "        report_to = \"none\",               \n",
    "    ),\n",
    ")\n",
    "\n",
    "total_steps = (len(train_dataset) // (4 * 4)) * 3\n",
    "print(f\"Total Estimated Steps: ~{total_steps}\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "print(\"Full Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbba50b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T21:56:59.468130Z",
     "iopub.status.busy": "2025-11-19T21:56:59.467882Z",
     "iopub.status.idle": "2025-11-19T22:20:02.727900Z",
     "shell.execute_reply": "2025-11-19T22:20:02.727086Z"
    },
    "papermill": {
     "duration": 1383.295617,
     "end_time": "2025-11-19T22:20:02.745750",
     "exception": false,
     "start_time": "2025-11-19T21:56:59.450133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Phase B: Running Fine-Tuned Inference...\n",
      "running inference on 200 samples...\n",
      " Phase B Scores: {'bleu': 0.3296, 'rouge1': 0.1301}\n",
      " Improvement (BLEU): 0.3006\n"
     ]
    }
   ],
   "source": [
    "print(\"Running Fine-Tuned Inference...\")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "preds_b, refs_b = inference_loop(model, tokenizer, test_dataset, limit=200)\n",
    "scores_b = calculate_metrics(preds_b, refs_b)\n",
    "\n",
    "print(f\"Fine-Tuned Scores: {scores_b}\")\n",
    "print(f\"Improvement (BLEU): {scores_b['bleu'] - scores_a['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5e147f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T22:20:02.779639Z",
     "iopub.status.busy": "2025-11-19T22:20:02.779387Z",
     "iopub.status.idle": "2025-11-19T22:27:52.316632Z",
     "shell.execute_reply": "2025-11-19T22:27:52.315693Z"
    },
    "papermill": {
     "duration": 469.555845,
     "end_time": "2025-11-19T22:27:52.318145",
     "exception": false,
     "start_time": "2025-11-19T22:20:02.762300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Converting to GGUF (q5_k_m)...\n",
      "Unsloth: Merging model weights to 16-bit format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad356586a92a48cead3d0a4737bb5b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/757 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9752670543864438ba51d0ac0feb47d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89af1055a5ff40ebbb7f56babeaa9c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c6e0aae11947d4b25e893d6b7c17e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:11<00:11, 11.48s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45d7389269d4c478839a38c53c3d4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:22<00:00, 11.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:44<00:00, 22.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/kaggle/working/model_gguf`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF f16 to ['q5_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: Updating system package directories\n",
      "Unsloth: All required system packages already installed!\n",
      "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
      "Unsloth: Cloning llama.cpp repository\n",
      "Unsloth: Install GGUF and other packages\n",
      "Unsloth: Successfully installed llama.cpp!\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into f16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['qwen2.5-3b-instruct.F16.gguf']\n",
      "Unsloth: [2] Converting GGUF f16 into q5_k_m. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['qwen2.5-3b-instruct.Q5_K_M.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model qwen2.5-3b-instruct.Q5_K_M.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n",
      " GGUF Saved.\n",
      " Cleaning Memory for Phase C...\n",
      "   - VRAM after cleanup: 0.65 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting to GGUF (q5_k_m)...\")\n",
    "save_path = \"model_q5_k_m.gguf\"\n",
    "\n",
    "try:\n",
    "    model.save_pretrained_gguf(\"model_gguf\", tokenizer, quantization_method = \"q5_k_m\")\n",
    "    print(\"GGUF Saved.\")\n",
    "    \n",
    "    print(\"Cleaning Memory...\")\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"VRAM after cleanup: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Quantization Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "defc01a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T22:27:52.357787Z",
     "iopub.status.busy": "2025-11-19T22:27:52.357532Z",
     "iopub.status.idle": "2025-11-19T22:28:27.822819Z",
     "shell.execute_reply": "2025-11-19T22:28:27.821818Z"
    },
    "papermill": {
     "duration": 35.487042,
     "end_time": "2025-11-19T22:28:27.824532",
     "exception": false,
     "start_time": "2025-11-19T22:27:52.337490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-build-core\r\n",
      "  Using cached scikit_build_core-0.11.6-py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (3.31.6)\r\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from scikit-build-core) (25.0)\r\n",
      "Requirement already satisfied: pathspec>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-build-core) (0.12.1)\r\n",
      "Using cached scikit_build_core-0.11.6-py3-none-any.whl (185 kB)\r\n",
      "Installing collected packages: scikit-build-core\r\n",
      "Successfully installed scikit-build-core-0.11.6\r\n",
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\r\n",
      "Collecting llama-cpp-python\r\n",
      "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu121/llama_cpp_python-0.3.16-cp311-cp311-linux_x86_64.whl (551.3 MB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m551.3/551.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.15.0)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\r\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\r\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-build-core cmake\n",
    "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce382c68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T22:28:27.878983Z",
     "iopub.status.busy": "2025-11-19T22:28:27.878721Z",
     "iopub.status.idle": "2025-11-19T22:34:14.456802Z",
     "shell.execute_reply": "2025-11-19T22:34:14.455892Z"
    },
    "papermill": {
     "duration": 346.634803,
     "end_time": "2025-11-19T22:34:14.486300",
     "exception": false,
     "start_time": "2025-11-19T22:28:27.851497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“± Phase C: Verifying GGUF on Edge Runtime...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running inference on 200 samples...\n",
      " Phase C Scores: {'bleu': 0.1606, 'rouge1': 0.0938}\n",
      " Quantization Loss (BLEU): 0.1690\n",
      "\n",
      " PIPELINE COMPLETE. Download 'model_gguf/model_q5_k_m.gguf' for your portfolio.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "print(\"Verifying GGUF on Edge Runtime...\")\n",
    "full_path = \"/kaggle/working/qwen2.5-3b-instruct.Q5_K_M.gguf\"\n",
    "\n",
    "try:\n",
    "    edge_model = Llama(\n",
    "        model_path=full_path,\n",
    "        n_gpu_layers=-1, \n",
    "        verbose=False,\n",
    "        n_ctx=2048\n",
    "    )\n",
    "    \n",
    "    preds_c, refs_c = inference_loop(edge_model, None, test_dataset, limit=200, is_gguf=True)\n",
    "    scores_c = calculate_metrics(preds_c, refs_c)\n",
    "    \n",
    "    print(f\"Verification Scores: {scores_c}\")\n",
    "    print(f\"Quantization Loss (BLEU): {scores_b['bleu'] - scores_c['bleu']:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Verification Failed: {e}\")\n",
    "\n",
    "print(\"Pipeline Complete. Ready for download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd08502b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T22:34:14.539919Z",
     "iopub.status.busy": "2025-11-19T22:34:14.539680Z",
     "iopub.status.idle": "2025-11-19T22:55:18.830658Z",
     "shell.execute_reply": "2025-11-19T22:55:18.829865Z"
    },
    "papermill": {
     "duration": 1264.34657,
     "end_time": "2025-11-19T22:55:18.858871",
     "exception": false,
     "start_time": "2025-11-19T22:34:14.512301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Phase 1: Loading Full Model (FP16) from 'model_gguf' folder...\n",
      "==((====))==  Unsloth 2025.11.3: Fast Qwen2 patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43736fdf1d440e98c266d308af9f13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    FP16 VRAM Usage: 5.77 GB\n",
      "    Running Inference on 200 samples...\n",
      "    FP16 Speed: 24.30 tokens/sec\n",
      "    FP16 Accuracy: {'bleu': 0.2978, 'rouge1': 0.1447}\n",
      "\n",
      "ğŸ”¹ Phase 2: Loading Quantized Model (GGUF Q5_K_M)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    GGUF VRAM Usage: ~2.57 GB (Est)\n",
      "    Running Inference on 200 samples...\n",
      "    GGUF Speed: 54.49 tokens/sec\n",
      "    GGUF Accuracy: {'bleu': 0.1993, 'rouge1': 0.0938}\n",
      "\n",
      "============================================================\n",
      "ğŸš€ FINAL BENCHMARK: Full Precision vs Quantization\n",
      "============================================================\n",
      "Metric               | FP16 (Full)     | GGUF (Q5)       | Impact         \n",
      "----------------------------------------------------------------------\n",
      "Memory (VRAM)        | 5.7655 GB        | 2.5720 GB        | 55.39% Smaller\n",
      "Inference Speed      | 24.30384 t/s        | 54.49194 t/s        | +124.21% Change\n",
      "Accuracy (BLEU)      | 0.2978          | 0.1993          | -0.09850 Delta\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "from llama_cpp import Llama\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "def calculate_metrics(preds, refs):\n",
    "    preds = [p if len(p) > 0 else \" \" for p in preds]\n",
    "    r = rouge.compute(predictions=preds, references=refs)\n",
    "    b = bleu.compute(predictions=preds, references=refs)\n",
    "    return {\"bleu\": round(b['bleu'], 4), \"rouge1\": round(r['rouge1'], 4)}\n",
    "\n",
    "def get_vram_gb():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated(0) / 1024**3\n",
    "    return 0\n",
    "\n",
    "def benchmark_model(model, tokenizer, prompts, refs, is_gguf=False):\n",
    "    times = []\n",
    "    token_counts = []\n",
    "    preds = []\n",
    "    \n",
    "    print(f\"Running Inference on {len(prompts)} samples...\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        start = time.time()\n",
    "        if not is_gguf:\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            gen_out = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=128, \n",
    "                use_cache=True, \n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            end = time.time()\n",
    "            generated_ids = gen_out[:, inputs.input_ids.shape[1]:]\n",
    "            num_tokens = generated_ids.shape[1]\n",
    "            output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            res = model(\n",
    "                prompt, \n",
    "                max_tokens=128, \n",
    "                stop=[\"<|im_end|>\"], \n",
    "                echo=False\n",
    "            )\n",
    "            end = time.time()\n",
    "            num_tokens = res['usage']['completion_tokens']\n",
    "            output_text = res['choices'][0]['text']\n",
    "            \n",
    "        times.append(end - start)\n",
    "        token_counts.append(num_tokens)\n",
    "        preds.append(output_text.strip())\n",
    "        \n",
    "    avg_speed = sum(token_counts) / sum(times)\n",
    "    metrics = calculate_metrics(preds, refs)\n",
    "    return avg_speed, metrics\n",
    "\n",
    "try:\n",
    "    subset = test_dataset.select(range(200)) \n",
    "    prompts = [row['text'].split(\"<|im_start|>assistant\")[0] + \"<|im_start|>assistant\\n\" for row in subset]\n",
    "    refs = [row['ground_truth'] for row in subset]\n",
    "except:\n",
    "    print(\"Test dataset missing. Using dummy data.\")\n",
    "    prompts = [\"<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\n\"] * 5\n",
    "    refs = [\"Hi\"] * 5\n",
    "\n",
    "print(\"Loading Full Model (FP16)...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "start_mem = get_vram_gb()\n",
    "\n",
    "model_fp16, tokenizer_fp16 = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"model_gguf\",  \n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.float16,\n",
    "    load_in_4bit = False,\n",
    ")\n",
    "FastLanguageModel.for_inference(model_fp16)\n",
    "\n",
    "fp16_mem = get_vram_gb() - start_mem\n",
    "print(f\"FP16 VRAM Usage: {fp16_mem:.2f} GB\")\n",
    "\n",
    "fp16_speed, fp16_acc = benchmark_model(model_fp16, tokenizer_fp16, prompts, refs, is_gguf=False)\n",
    "print(f\"FP16 Speed: {fp16_speed:.2f} tokens/sec\")\n",
    "print(f\"FP16 Accuracy: {fp16_acc}\")\n",
    "\n",
    "del model_fp16, tokenizer_fp16\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Loading Quantized Model (GGUF)...\")\n",
    "gguf_path = \"qwen2.5-3b-instruct.Q5_K_M.gguf\" \n",
    "\n",
    "if not os.path.exists(gguf_path):\n",
    "    if os.path.exists(f\"/kaggle/working/{gguf_path}\"):\n",
    "         gguf_path = f\"/kaggle/working/{gguf_path}\"\n",
    "    else:\n",
    "        print(f\"Error: GGUF file not found at {gguf_path}\")\n",
    "\n",
    "model_gguf = Llama(\n",
    "    model_path=gguf_path,\n",
    "    n_gpu_layers=-1,\n",
    "    verbose=False,\n",
    "    n_ctx=2048\n",
    ")\n",
    "\n",
    "gguf_size = os.path.getsize(gguf_path) / 1024**3\n",
    "gguf_mem = gguf_size + 0.5 \n",
    "print(f\"GGUF VRAM Usage: ~{gguf_mem:.2f} GB (Est)\")\n",
    "\n",
    "gguf_speed, gguf_acc = benchmark_model(model_gguf, None, prompts, refs, is_gguf=True)\n",
    "print(f\"GGUF Speed: {gguf_speed:.2f} tokens/sec\")\n",
    "print(f\"GGUF Accuracy: {gguf_acc}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL BENCHMARK: Full Precision vs Quantization\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} | {'FP16 (Full)':<15} | {'GGUF (Q5)':<15} | {'Impact':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "mem_delta = ((fp16_mem - gguf_mem) / fp16_mem) * 100\n",
    "print(f\"{'Memory (VRAM)':<20} | {fp16_mem:.4f} GB        | {gguf_mem:.4f} GB        | {mem_delta:.2f}% Smaller\")\n",
    "\n",
    "speed_delta = ((gguf_speed - fp16_speed) / fp16_speed) * 100\n",
    "print(f\"{'Inference Speed':<20} | {fp16_speed:.5f} t/s        | {gguf_speed:.5f} t/s        | {speed_delta:+.2f}% Change\")\n",
    "\n",
    "acc_delta = gguf_acc['bleu'] - fp16_acc['bleu']\n",
    "print(f\"{'Accuracy (BLEU)':<20} | {fp16_acc['bleu']:<15} | {gguf_acc['bleu']:<15} | {acc_delta:+.5f} Delta\")\n",
    "print(\"-\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8781059,
     "sourceId": 13792717,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6628.055595,
   "end_time": "2025-11-19T22:55:21.906071",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T21:04:53.850476",
   "version": "2.6.0"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
